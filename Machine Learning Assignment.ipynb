{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35c8a5a-3444-4ef0-95d5-faaafca3298f",
   "metadata": {},
   "source": [
    "# Machine Learning Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed289d5-a904-40a6-b7cd-02e375057d16",
   "metadata": {},
   "source": [
    "### Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35e9bd8-2d44-4444-a93f-979d9012697e",
   "metadata": {},
   "source": [
    "#### Overfitting\n",
    "Overfitting is when Data trained from training dataset has High Accuracy level but when tested results in low accuracy.\n",
    "Overfitting of data is considered a bad condition\n",
    "In this case we can say that there is Low Bias and High Variance.\n",
    "\n",
    "#### Underfitting\n",
    "Underfitting is when the Data trained from training dataset gives Low accuracy  and when tested also results in low accuracy.\n",
    "Underfitting of data is also a bad condition.\n",
    "In this case we can say that there is a High Bias and High Variance.\n",
    "\n",
    "#### Consequences and Solution\n",
    "In both the case, the modelis not performing well.\n",
    "Hence, we need to generalize the model.\n",
    "A generailsed model has Low Bias and Low Variance.\n",
    "In this case the Accuracy is high for the Train as well as Test Dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c602949b-41a0-4949-a196-b5e2b92e92f9",
   "metadata": {},
   "source": [
    "### Q2: How can we reduce overfitting? Explain in brief."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce3d740-d975-4266-8b46-da19e56f8de9",
   "metadata": {},
   "source": [
    "#### Cross-validation\n",
    "Cross-validation is a powerful preventative measure against overfitting.\n",
    "The idea is clever: Use your initial training data to generate multiple mini train-test splits. Use these splits to tune your model.\n",
    "\n",
    "#### Train with more data\n",
    "It won’t work every time, but training with more data can help algorithms detect the signal better.If we just add more noisy data, this technique won’t help. That’s why you should always ensure your data is clean and relevant.\n",
    "\n",
    "#### Remove features\n",
    "Some algorithms have built-in feature selection.\n",
    "For those that don’t, you can manually improve their generalizability by removing irrelevant input features.\n",
    "An interesting way to do so is to tell a story about how each feature fits into the model. This is like the data scientist’s spin on software engineer’s rubber duck debugging technique, where they debug their code by explaining it, line-by-line, to a rubber duck.\n",
    "If anything doesn’t make sense, or if it’s hard to justify certain features, this is a good way to identify them.\n",
    "\n",
    "#### Early stopping\n",
    "When you’re training a learning algorithm iteratively, you can measure how well each iteration of the model performs.\n",
    "Up until a certain number of iterations, new iterations improve the model. After that point, however, the model’s ability to generalize can weaken as it begins to overfit the training data.\n",
    "Early stopping refers stopping the training process before the learner passes that point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4362784f-5b2f-4457-8895-6654e1fdc76d",
   "metadata": {},
   "source": [
    "### Q3: Explain underfitting. List scenarios where underfitting can occur in ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8c480d-6fcd-4baf-9801-0e0d4951cf90",
   "metadata": {},
   "source": [
    "#### Underfitting\n",
    "Underfitting is a scenario in data science where a data model is unable to capture the relationship between the input and output variables accurately, generating a high error rate on both the training set and unseen data. \n",
    "\n",
    "#### It occurs when a model is too simple, which can be a result of a model needing more training time, more input features, or less regularization. \n",
    "\n",
    "Like overfitting, when a model is underfitted, it cannot establish the dominant trend within the data, resulting in training errors and poor performance of the model. If a model cannot generalize well to new data, then it cannot be leveraged for classification or prediction tasks. Generalization of a model to new data is ultimately what allows us to use machine learning algorithms every day to make predictions and classify data.\n",
    "\n",
    "High bias and low variance are good indicators of underfitting. Since this behavior can be seen while using the training dataset, underfitted models are usually easier to identify than overfitted ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d274a51a-6e0e-46eb-86a6-8b6d1833a396",
   "metadata": {},
   "source": [
    "### Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d082519-a527-4696-ba58-c183b6d80448",
   "metadata": {},
   "source": [
    "* #### What is bias?\n",
    "\n",
    "Bias is the difference between the average prediction of our model and the correct value which we are trying to predict. Model with high bias pays very little attention to the training data and oversimplifies the model. It always leads to high error on training and test data.\n",
    "\n",
    "* #### What is variance?\n",
    "\n",
    "Variance is the variability of model prediction for a given data point or a value which tells us spread of our data. Model with high variance pays a lot of attention to training data and does not generalize on the data which it hasn’t seen before. As a result, such models perform very well on training data but has high error rates on test data.\n",
    "\n",
    "* #### Relation and effects\n",
    "\n",
    "If our model is too simple and has very few parameters then it may have high bias and low variance. On the other hand if our model has large number of parameters then it’s going to have high variance and low bias. So we need to find the right/good balance without overfitting and underfitting the data.\n",
    "\n",
    "This tradeoff in complexity is why there is a tradeoff between bias and variance. An algorithm can’t be more complex and less complex at the same time.\n",
    "An optimal balance of bias and variance would never overfit or underfit the model.\n",
    "\n",
    "Therefore understanding bias and variance is critical for understanding the behavior of prediction models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8b846c-7e02-4c62-b418-9ca5f552d1f2",
   "metadata": {},
   "source": [
    "### Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models. How can you determine whether your model is overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ee596d-875a-4db3-815d-26d1ebc2195d",
   "metadata": {},
   "source": [
    "Your model is underfitting the training data when the model performs poorly on the training data. This\n",
    "is because the model is unable to capture the relationship between the input examples (often called X)\n",
    "and the target values (often called Y). Your model is overfitting your training data when you see that\n",
    "the model performs well on the training data but does not perform well on the evaluation data. This is\n",
    "because the model is memorizing the data it has seen and is unable to generalize to unseen examples.\n",
    "Poor performance on the training data could be because the model is too simple (the input features are\n",
    "not expressive enough) to describe the target well. Performance can be improved by increasing model\n",
    "flexibility.\n",
    "\n",
    "#### To increase model flexibility, try the following:\n",
    "\n",
    "• Add new domain-specific features and more feature Cartesian products, and change the types of\n",
    "feature processing used (e.g., increasing n-grams size)\n",
    "\n",
    "• Decrease the amount of regularization used\n",
    "\n",
    "\n",
    "If your model is overfitting the training data, it makes sense to take actions that reduce model flexibility.\n",
    "\n",
    "#### To reduce model flexibility, try the following:\n",
    "\n",
    "• Feature selection: consider using fewer feature combinations, decrease n-grams size, and decrease the\n",
    "number of numeric attribute bins.\n",
    "\n",
    "• Increase the amount of regularization used.\n",
    "\n",
    "Accuracy on training and test data could be poor because the learning algorithm did not have enough\n",
    "data to learn from. \n",
    "\n",
    "You could improve performance by doing the following:\n",
    "\n",
    "• Increase the amount of training data examples.\n",
    "\n",
    "\n",
    "• Increase the number of passes on the existing training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceafdfb1-898e-450f-8297-2df0a9eca784",
   "metadata": {},
   "source": [
    "### Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4421105f-5dc5-4c03-931a-29becab63614",
   "metadata": {},
   "source": [
    "Bias and variance are two important concepts in machine learning that relate to the performance and generalization ability of models. Let's compare and contrast bias and variance:\n",
    "\n",
    "1. Bias:\n",
    "   - Bias refers to the error introduced by approximating a real-world problem with a simplified model.\n",
    "   - A model with high bias oversimplifies the problem, leading to underfitting and poor performance on both the training and test data.\n",
    "   - Examples of high bias models include linear regression with few features, models with low complexity, or decision trees with shallow depth.\n",
    "   - High bias models tend to have high training and test error, indicating a lack of capability to capture the underlying patterns in the data.\n",
    "\n",
    "2. Variance:\n",
    "   - Variance refers to the sensitivity of a model to the fluctuations in the training data.\n",
    "   - A model with high variance is overly complex and tends to overfit the training data, resulting in poor performance on new, unseen data.\n",
    "   - Examples of high variance models include deep neural networks with many layers, decision trees with high depth, or models with a large number of features.\n",
    "   - High variance models typically have low training error but high test error, as they are too flexible and memorize noise or specific patterns in the training data that do not generalize well.\n",
    "\n",
    "Performance differences between high bias and high variance models:\n",
    "- High bias models have a tendency to underfit the data, leading to both high training and test error.\n",
    "- High variance models have a tendency to overfit the data, resulting in low training error but high test error.\n",
    "- In terms of model complexity, high bias models are generally simpler and have lower capacity, while high variance models are more complex and have higher capacity.\n",
    "- Increasing the complexity or capacity of a model can reduce bias but increase variance, and vice versa.\n",
    "- The goal in machine learning is to strike a balance between bias and variance to achieve good generalization performance.\n",
    "\n",
    "To summarize, bias and variance represent different sources of error in machine learning models. High bias models underfit the data, while high variance models overfit the data. Achieving the right balance between bias and variance is essential for building models that generalize well to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7b8804-af5f-4f68-b70a-9290b1482b50",
   "metadata": {},
   "source": [
    "### Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f9d52d-1b0c-4e5e-9eba-af14e214f677",
   "metadata": {},
   "source": [
    "Regularization is a technique used in machine learning to prevent overfitting, which occurs when a model becomes too complex and starts to memorize noise or specific patterns in the training data that do not generalize well to new, unseen data. Regularization methods add additional constraints or penalties to the model's optimization process to encourage simpler and more generalized solutions. Here are some common regularization techniques:\n",
    "\n",
    "#### L1 Regularization (Lasso):\n",
    "\n",
    "L1 regularization adds a penalty term to the loss function that is proportional to the absolute value of the model's coefficients.\n",
    "This regularization technique encourages sparsity by shrinking some coefficients to exactly zero, effectively performing feature selection.\n",
    "L1 regularization can help eliminate less important features from the model, making it more interpretable and reducing complexity.\n",
    "\n",
    "\n",
    "#### L2 Regularization (Ridge):\n",
    "\n",
    "L2 regularization adds a penalty term to the loss function that is proportional to the square of the model's coefficients.\n",
    "This regularization technique encourages small but non-zero coefficients for all features.\n",
    "L2 regularization can help reduce the impact of outliers and the model's sensitivity to individual data points.\n",
    "\n",
    "#### Elastic Net Regularization:\n",
    "\n",
    "Elastic Net regularization combines both L1 and L2 regularization.\n",
    "It adds a penalty term that is a linear combination of the L1 and L2 penalty terms.\n",
    "Elastic Net regularization can capture both sparse solutions (L1) and deal with correlated features (L2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7565b282-5511-42b9-9e5f-fa69037cf9cc",
   "metadata": {},
   "source": [
    "## The End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
